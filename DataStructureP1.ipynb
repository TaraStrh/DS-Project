{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWsbnDZsrttG",
        "outputId": "25c48d0c-32fe-4e3c-b55f-38bd251e2ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XB7r43A47qyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d0cf1d-4998-4083-a276-ce139380f630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/DS_Project.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LWFD9Gb8AcK",
        "outputId": "94ab8c27-aab1-40e4-bcb9-216f867884e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "پروژه ساختمان داده.pdf:  mismatching \"local\" filename (┘╛╪▒┘И┌Ш┘З ╪│╪з╪о╪к┘Е╪з┘Ж ╪п╪з╪п┘З.pdf),\n",
            "         continuing with \"central\" filename version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"/content/data\"\n",
        "file_list = os.listdir(folder_path)\n",
        "#sorted(file_list)\n",
        "doc = \"document_\"\n",
        "nums = []\n",
        "for x in file_list:\n",
        "  num = x[len(doc):x.find('.')]\n",
        "  nums.append(int(num))\n",
        "text_list = []\n",
        "for num in sorted(nums):\n",
        "    file_name = f\"{doc}{num}.txt\"\n",
        "    if file_name.endswith(\".txt\"):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        with open(file_path, \"r\") as f:\n",
        "            text = f.read()\n",
        "            text_list.append(text)\n",
        "\n",
        "print(text_list[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfsJYms6CZYG",
        "outputId": "df8cd180-64ce-4299-cf5c-809c2d8a4e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sydney, New South Wales, Australia is located in a coastal basin bordered by the Pacific Ocean to the east, the Blue Mountains to the west, the Hawkesbury River to the north and the Woronora Plateau to the south. The Sydney Statistical Division, used for census data, is the unofficial metropolitan area and covers 12,145 km² (4,689 mi²). This area includes the Central Coast and Blue Mountains as well as broad swathes of national park and other non-urban land.\n",
            "This itinerary will have you crossing the country to take in the Great Barrier Reef, Australia’s iconic reef in Queensland, before heading to Western Australia to see the breath taking Ningaloo Reef, one of Australia’s best kept secrets. View more information. 10 day-Sydney, rock and reef. It’s easy to see why Hamilton Island is one of the most popular spots for a getaway on the Great Barrier Reef. With palm-fringed beaches, top restaurants and stylish resorts, there’s plenty to do on land, while those keen to explore the clear waters of the Whitsundays will be richly rewarded.\n",
            "The Sydney central business district, Sydney harbour and outer suburbs from the West. North Sydney 's commercial district. The extensive area covered by urban Sydney is formally divided into more than 300 suburbs for addressing and postal purposes, and administered as 38 local government areas. The Sydney Statistical Division, used for census data, is the unofficial metropolitan area and covers 12,145 km² (4,689 mi²). This area includes the Central Coast and Blue Mountains as well as broad swathes of national park and other non-urban land.\n",
            "1 Taxis to the city centre should cost approximately $40 (including tolls), and more to other Sydney destinations (The Rocks $40-45, North Sydney $45, Manly $50, Parramatta $80-100 etc.) You can expect to pay a $3.80 airport taxi levy and a $5.50 Eastern Distributor toll on top of the metered fare. Newtown in Sydney's inner-west (approx 4km from the CBD) is renowned for its inexpensive cafes and restaurants on King St, in particular Thai food. It is highly popular among students from the nearby Sydney University.\n",
            "Sydney is the capital city of the Australian state of New South Wales, and Australia's largest city. A week in Sydney will help you see many of the sights of Sydney and its surrounds, and understand the city and its culture. If you're up for exploring the area by bike (one of the best ways to do so as much of it is parkland), take the train to Concord West station on the Northern Line (red line on the Sydney Trains map-about 20-25 minutes from the city on a direct train).\n",
            "Sydney Attractions. Sydney is home to some of Australia’s most iconic attractions. The Sydney Opera House is a thriving hub of art, culture and history. 1km (return) - 1.5 hour (each way)Step out of your car and into the past. This short walk, perfect for walking with children and for visitors with ties to the local area, pulls you back thro... http://www.nationalparks.nsw.gov.au/scheyville-national-park/migrant-heritage-walk/walking.\n",
            "Sydney lies on a submergent coastline, where the ocean level has risen to flood deep river valleys (rias) carved in the sandstone. There are more than 70 harbour and ocean beaches, including the famous Bondi Beach, in the urban area. Sydney's urban area covers 1,788 km² (690 mi²). The Sydney Statistical Division, used for census data, is the unofficial metropolitan area and covers 12,145 km² (4,689 mi²). This area includes the Central Coast and Blue Mountains as well as broad swathes of national park and other non-urban land.\n",
            "On your right across College Street, in the sandstone building on the corner, is the Australian Museum [3] ($12 adult/$6 children, $30 family (2+2)). This museum, which focuses on natural history, is worth a visit in its own right if you have more time in Sydney and will take a couple of hours to explore. If you're up for exploring the area by bike (one of the best ways to do so as much of it is parkland), take the train to Concord West station on the Northern Line (red line on the Sydney Trains map-about 20-25 minutes from the city on a direct train).\n",
            "The Rocks. This is a complete listing of the suburbs and localities in the greater Sydney area in alphabetical order. Sydney has about 38 local government areas, each consisting of several suburbs (suburbs in Australia are purely geographical, not political, divisions). See table below, Category:Suburbs of Sydney and Category:Local government areas in Sydney. Suburbs are listed here if they are inside the Sydney metro area, and are listed in the Geographical Names Register as being suburbs.\n",
            "Sydney is a very large city, and we haven't spent much time outside of the inner suburbs, apart from what you would have seen on the way to the Blue Mountains. On Day 5 we'll explore one of Sydney's parkland areas in the Sydney Olympic Park at Homebush Bay [63] . If you're up for exploring the area by bike (one of the best ways to do so as much of it is parkland), take the train to Concord West station on the Northern Line (red line on the Sydney Trains map-about 20-25 minutes from the city on a direct train).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_list[5].split('\\n')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3x0_eDXLsF0",
        "outputId": "f1ae06c3-54d1-455a-d6f0-1cf438f68f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get the Latest health and medical information delivered direct to your inbox! The lungs are a pair of spongy, air-filled organs located on either side of the chest (thorax). The trachea (windpipe) conducts inhaled air into the lungs through its tubular branches, called bronchi. The bronchi then divide into smaller and smaller branches (bronchioles), finally becoming microscopic. Between the alveoli is a thin layer of cells called the interstitium, which contains blood vessels and cells that help support the alveoli. The lungs are covered by a thin tissue layer called the pleura. The same kind of thin tissue lines the inside of the chest cavity -- also called pleura.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Part 0:"
      ],
      "metadata": {
        "id": "dzkvj4vc5-eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  words = word_tokenize(text)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_words = [word for word in words if word.isalnum() or word.isdigit() and word not in stop_words]\n",
        "  return filtered_words"
      ],
      "metadata": {
        "id": "5YodSXTpwsrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizing:"
      ],
      "metadata": {
        "id": "EHRW3wXT6Hx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenized_list = [preprocess(text) for text in text_list]\n"
      ],
      "metadata": {
        "id": "kDMQjg5nLQn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Tokenized_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2z53Y6u6w7F",
        "outputId": "5e496b9c-4461-4214-9fdd-10f496bc50ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['since', '2007', 'the', 'rba', 'outstanding', 'reputation', 'has', 'been', 'affected', 'by', 'the', 'or', 'npa', 'scandal', 'these', 'rba', 'subsidiaries', 'were', 'involved', 'in', 'bribing', 'overseas', 'officials', 'so', 'that', 'australia', 'might', 'win', 'lucrative', 'contracts', 'the', 'assets', 'of', 'the', 'bank', 'include', 'the', 'gold', 'and', 'foreign', 'exchange', 'reserves', 'of', 'australia', 'which', 'is', 'estimated', 'to', 'have', 'a', 'net', 'worth', 'of', 'a', '101', 'billion', 'nearly', '94', 'of', 'the', 'rba', 'employees', 'work', 'at', 'its', 'headquarters', 'in', 'sydney', 'new', 'south', 'wales', 'and', 'at', 'the', 'business', 'resumption', 'site', 'the', 'reserve', 'bank', 'of', 'australia', 'rba', 'came', 'into', 'being', 'on', '14', 'january', '1960', 'as', 'australia', 'central', 'bank', 'and', 'banknote', 'issuing', 'authority', 'when', 'the', 'reserve', 'bank', 'act', '1959', 'removed', 'the', 'central', 'banking', 'functions', 'from', 'the', 'commonwealth', 'bank', 'the', 'assets', 'of', 'the', 'bank', 'include', 'the', 'gold', 'and', 'foreign', 'exchange', 'reserves', 'of', 'australia', 'which', 'is', 'estimated', 'to', 'have', 'a', 'net', 'worth', 'of', 'a', '101', 'billion', 'nearly', '94', 'of', 'the', 'rba', 'employees', 'work', 'at', 'its', 'headquarters', 'in', 'sydney', 'new', 'south', 'wales', 'and', 'at', 'the', 'business', 'resumption', 'site', 'rba', 'recognized', 'with', 'the', '2014', 'microsoft', 'us', 'regional', 'partner', 'of', 'the', 'by', 'pr', 'newswire', 'contract', 'awarded', 'for', 'supply', 'and', 'support', 'the', 'securitisations', 'system', 'used', 'for', 'risk', 'management', 'and', 'analysis', 'the', 'inner', 'workings', 'of', 'a', 'rebuildable', 'atomizer', 'are', 'surprisingly', 'simple', 'the', 'coil', 'inside', 'the', 'rba', 'is', 'made', 'of', 'some', 'type', 'of', 'resistance', 'wire', 'normally', 'kanthal', 'or', 'nichrome', 'when', 'a', 'current', 'is', 'applied', 'to', 'the', 'coil', 'resistance', 'wire', 'it', 'heats', 'up', 'and', 'the', 'heated', 'coil', 'then', 'vaporizes', 'the', 'eliquid', '1', 'the', 'bottom', 'feed', 'rba', 'is', 'perhaps', 'the', 'easiest', 'of', 'all', 'rba', 'types', 'to', 'build', 'maintain', 'and', 'use', '2', 'it', 'is', 'filled', 'from', 'below', 'much', 'like', 'bottom', 'coil', 'clearomizer', '3', 'bottom', 'feed', 'rbas', 'can', 'utilize', 'cotton', 'instead', 'of', 'silica', 'for', 'the', 'wick', '4', 'the', 'genesis', 'or', 'genny', 'is', 'a', 'top', 'feed', 'rba', 'that', 'utilizes', 'a', 'short', 'woven', 'mesh', 'wire', 'also', 'known', 'as', 'rba', 'is', 'a', 'disciplined', 'way', 'of', 'thinking', 'and', 'taking', 'action', 'that', 'communities', 'can', 'use', 'to', 'improve', 'the', 'lives', 'of', 'children', 'youth', 'families', 'adults', 'and', 'the', 'community', 'as', 'a', 'whole', 'rba', 'is', 'also', 'used', 'by', 'organizations', 'to', 'improve', 'the', 'performance', 'of', 'their', 'programs', 'rba', 'improves', 'the', 'lives', 'of', 'children', 'families', 'and', 'communities', 'and', 'the', 'performance', 'of', 'programs', 'because', 'rba', '1', 'gets', 'from', 'talk', 'to', 'action', 'quickly', '2', 'is', 'a', 'simple', 'common', 'sense', 'process', 'that', 'everyone', 'can', 'understand', '3', 'helps', 'groups', 'to', 'surface', 'and', 'challenge', 'assumptions', 'that', 'can', 'be', 'barriers', 'to', 'innovation', 'also', 'known', 'as', 'rba', 'is', 'a', 'disciplined', 'way', 'of', 'thinking', 'and', 'taking', 'action', 'that', 'communities', 'can', 'use', 'to', 'improve', 'the', 'lives', 'of', 'children', 'youth', 'families', 'adults', 'and', 'the', 'community', 'as', 'a', 'whole', 'rba', 'is', 'also', 'used', 'by', 'organizations', 'to', 'improve', 'the', 'performance', 'of', 'their', 'programs', 'creating', 'community', 'impact', 'with', 'rba', 'community', 'impact', 'focuses', 'on', 'conditions', 'of', 'for', 'children', 'families', 'and', 'the', 'community', 'as', 'a', 'whole', 'that', 'a', 'group', 'of', 'leaders', 'is', 'working', 'collectively', 'to', 'improve', 'for', 'example', 'residents', 'with', 'good', 'jobs', 'children', 'ready', 'for', 'school', 'or', 'a', 'safe', 'and', 'clean', 'neighborhood', 'rba', 'uses', 'a', 'process', 'to', 'help', 'communities', 'and', 'organizations', 'get', 'beyond', 'talking', 'about', 'problems', 'to', 'taking', 'action', 'to', 'solve', 'problems', 'it', 'is', 'a', 'simple', 'common', 'sense', 'framework', 'that', 'everyone', 'can', 'understand', 'rba', 'starts', 'with', 'ends', 'and', 'works', 'backward', 'towards', 'means', 'the', 'end', 'or', 'difference', 'you', 'are', 'trying', 'to', 'make', 'looks', 'slightly', 'different', 'if', 'you', 'are', 'working', 'on', 'a', 'broad', 'community', 'level', 'or', 'are', 'focusing', 'on', 'your', 'specific', 'program', 'or', 'organization', 'rba', 'improves', 'the', 'lives', 'of', 'children', 'families', 'and', 'communities', 'and', 'the', 'performance', 'of', 'programs', 'because', 'rba', '1', 'gets', 'from', 'talk', 'to', 'action', 'quickly', '2', 'is', 'a', 'simple', 'common', 'sense', 'process', 'that', 'everyone', 'can', 'understand', '3', 'helps', 'groups', 'to', 'surface', 'and', 'challenge', 'assumptions', 'that', 'can', 'be', 'barriers', 'to', 'innovation', 'netiq', 'identity', 'manager', 'authentication', 'rba', 'is', 'a', 'method', 'of', 'applying', 'varying', 'levels', 'of', 'stringency', 'to', 'authentication', 'processes', 'based', 'on', 'the', 'likelihood', 'that', 'access', 'to', 'a', 'given', 'system', 'could', 'result', 'in', 'its', 'being', 'compromised', 'authentication', 'can', 'be', 'categorized', 'as', 'either', 'or', 'rba', 'processes', 'employ', 'the', 'same', 'authentication', 'for', 'every', 'session', 'initiated', 'by', 'a', 'given', 'user', 'the', 'exact', 'credentials', 'that', 'the', 'site', 'demands', 'depend', 'on', 'who', 'the', 'user', 'is', 'a', 'rebuildable', 'atomizer', 'rba', 'often', 'referred', 'to', 'as', 'simply', 'a', 'rebuildable', 'is', 'just', 'a', 'special', 'type', 'of', 'atomizer', 'used', 'in', 'the', 'vape', 'pen', 'and', 'mod', 'industry', 'that', 'connects', 'to', 'a', 'personal', 'vaporizer', '1', 'the', 'bottom', 'feed', 'rba', 'is', 'perhaps', 'the', 'easiest', 'of', 'all', 'rba', 'types', 'to', 'build', 'maintain', 'and', 'use', '2', 'it', 'is', 'filled', 'from', 'below', 'much', 'like', 'bottom', 'coil', 'clearomizer', '3', 'bottom', 'feed', 'rbas', 'can', 'utilize', 'cotton', 'instead', 'of', 'silica', 'for', 'the', 'wick', '4', 'the', 'genesis', 'or', 'genny', 'is', 'a', 'top', 'feed', 'rba', 'that', 'utilizes', 'a', 'short', 'woven', 'mesh', 'wire', 'get', 'to', 'know', 'us', 'rba', 'is', 'a', 'digital', 'and', 'technology', 'consultancy', 'with', 'roots', 'in', 'strategy', 'design', 'and', 'technology', 'our', 'team', 'of', 'specialists', 'help', 'progressive', 'companies', 'deliver', 'modern', 'digital', 'experiences', 'backed', 'by', 'proven', 'technology', 'engineering']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenized_set = [set(l) for l in Tokenized_list]\n"
      ],
      "metadata": {
        "id": "fBuCp3g_SI0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = set()\n",
        "for i in Tokenized_set:\n",
        "    V.update(i)"
      ],
      "metadata": {
        "id": "pJpQYaQdRkMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDF = dict()\n",
        "for text in Tokenized_set:\n",
        "  for word in text:\n",
        "    if word in IDF.keys():\n",
        "      IDF[word] += 1\n",
        "    else:\n",
        "      IDF[word] = 1"
      ],
      "metadata": {
        "id": "4aq9-k-qQ--r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part1"
      ],
      "metadata": {
        "id": "D7BRi06evVwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def tf_idf(ind):\n",
        "  tf = Counter(Tokenized_list[ind])\n",
        "\n",
        "  w = []\n",
        "  for i in Tokenized_list[ind]:\n",
        "    w.append(np.log10(1 + tf[i]) * np.log10(float(len(Tokenized_list)) /(1.0 + float(IDF[i])) ))\n",
        "\n",
        "  #print(idf)\n",
        "  return w"
      ],
      "metadata": {
        "id": "BghSRvcsPIpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Splited_list = [text.split(\"\\n\") for text in text_list]\n",
        "print(Splited_list[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhM8k3a1ZNks",
        "outputId": "74341f47-b17e-4fb9-8525-06c9d5b45fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def par_tf_idf(ind , npar):\n",
        "  tf = Counter(preprocess(Splited_list[ind][npar]))\n",
        "\n",
        "  w = []\n",
        "  for i in preprocess(Splited_list[ind][npar]):\n",
        "    if IDF[i] == 0:\n",
        "      print(i)\n",
        "      continue\n",
        "    w.append(np.log10(1 + tf[i]) * np.log10(float(len(Tokenized_list)) / (1 + float(IDF[i])) ))\n",
        "\n",
        "\n",
        "  #print(idf)\n",
        "  return w"
      ],
      "metadata": {
        "id": "3L7hIosGsowZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(Splited_list[3][2]))\n",
        "print(par_tf_idf(3 , 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFQFG6Tks2jl",
        "outputId": "88f6fde2-f18b-4c39-f60b-05145a85967e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'right', 'Naproxen', 'dosage', '.', 'Naproxen', 'dosage', 'recommendations', 'for', 'treating', 'pain', 'in', 'osteoarthritis', ',', 'vary', 'between', '500', 'mg', 'and', '1000', 'mg', 'daily', '.', 'The', 'maximum', 'Naproxen', 'dosage', 'per', 'day', 'is', '1500', 'mg', '.', 'In', 'order', 'to', 'avoid', 'any', 'stomach', 'upset', 'is', 'better', 'to', 'take', 'your', 'daily', 'dosage', 'of', 'Naproxen', 'after', 'meals', 'or', 'with', 'a', 'glass', 'of', 'milk', '.', 'The', 'best', 'dosage', 'for', 'arthritis', 'is', 'the', 'same', 'as', 'in', 'case', 'of', 'osteoarthritis', ',', 'and', 'it', 'ranges', 'between', '500', 'mg', 'to']\n",
            "[0.11354335715356942, 0.937201152630621, 14.795914760555197, 10.38382333586252, 0.019811747589856078, 14.795914760555197, 10.38382333586252, 2.233595838690381, 0.08935396327463646, 1.791567329364213, 1.1962785148282364, 0.009923444652015551, 4.7915652654366205, 0.010239780875739466, 0.9490106066293962, 0.9258406468004896, 2.6547135259255557, 7.243196784881897, 0.0022786438408721387, 1.5488365282902405, 7.243196784881897, 2.55048288031294, 0.019811747589856078, 0.11354335715356942, 1.3603230245600988, 14.795914760555197, 10.38382333586252, 0.6816948685787814, 0.844915678272378, 0.020643602091210533, 2.043840255327417, 7.243196784881897, 0.019811747589856078, 0.25585657779035265, 0.9802279553991339, 0.011331882179061527, 1.5083668923251943, 0.517049394635566, 1.4964903730787056, 2.2533744868652015, 0.020643602091210533, 1.055427321575854, 0.011331882179061527, 0.7479327420029793, 0.31272949342987594, 2.55048288031294, 10.38382333586252, 0.003078377279957194, 14.795914760555197, 0.6398319707126008, 1.9157899990635416, 0.06043951714687877, 0.06983423431642898, 0.002037228271899201, 1.7457023534714948, 0.003078377279957194, 1.5454687008379615, 0.019811747589856078, 0.11354335715356942, 0.8615796657967766, 10.38382333586252, 0.08935396327463646, 1.9079902150499832, 0.020643602091210533, 0.0003388750968576977, 0.6845421623071271, 0.058318404400738225, 0.009923444652015551, 1.1388328502897516, 0.003078377279957194, 4.7915652654366205, 0.010239780875739466, 0.0022786438408721387, 0.12601420065705307, 1.2392847136608287, 0.9258406468004896, 2.6547135259255557, 7.243196784881897, 0.011331882179061527]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 1.1:"
      ],
      "metadata": {
        "id": "ubelZuBKUkD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VocabIndex = {word: index for index, word in enumerate(V)}"
      ],
      "metadata": {
        "id": "fKE05yASiaLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mapped_list = []\n",
        "for Tl in Tokenized_list:\n",
        "  ml = [VocabIndex[word] for word in Tl]\n",
        "  Mapped_list.append(ml)\n"
      ],
      "metadata": {
        "id": "WvUG0L5EihC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Mapped_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNlkX-0LZd7S",
        "outputId": "006636be-cefd-4464-8c6f-6ba45dae52e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[158823, 389299, 375123, 133848, 1703, 56644, 389695, 405760, 34436, 299892, 173109, 363255, 133848, 390384, 219106, 104532, 149266, 286453, 227187, 39458, 1703, 425395, 291458, 25263, 365238, 67651, 264604, 126624, 356568, 396578, 128048, 146467, 148202, 326072, 40097, 56675, 227187, 9951, 131310, 196270, 133848, 342391, 387421, 133848, 267888, 408310, 49338, 181147, 60105, 196270, 128048, 375123, 409218, 227089, 266686, 331324, 30954, 78031, 394437, 285722, 196270, 410316, 182267, 103079, 228484, 227187, 161986, 257133, 375542, 196270, 133848, 1703, 56644, 310397, 160869, 119307, 375349, 146647, 365238, 260520, 375123, 391407, 205803, 378871, 408310, 119307, 133848, 163556, 222614, 423629, 227187, 9951, 369623, 235652, 196270, 128048, 19233, 1703, 308713, 434982, 292727, 353638, 86747, 36642, 81309, 115334, 117302, 128048, 56644, 279640, 342391, 408310, 225460, 374508, 324639, 375123, 175864, 133848, 369623, 235652, 250585, 146473, 2581, 133848, 279640, 438010, 403419, 87063, 133848, 101355, 235652, 227187, 9951, 131310, 196270, 133848, 342391, 387421, 133848, 267888, 408310, 49338, 181147, 60105, 196270, 128048, 375123, 409218, 227089, 266686, 331324, 30954, 78031, 394437, 285722, 196270, 410316, 182267, 103079, 228484, 227187, 161986, 257133, 375542, 196270, 133848, 1703, 56644, 310397, 160869, 119307, 375349, 146647, 365238, 260520, 375123, 391407, 205803, 378871, 408310, 119307, 133848, 163556, 222614, 423629, 227187, 1703, 270471, 12457, 133848, 264431, 9059, 162780, 350631, 235719, 196270, 133848, 187682, 363255, 391524, 357286, 227187, 264874, 351521, 19580, 171761, 408310, 43929, 133848, 227187, 171019, 39993, 389351, 19580, 271223, 189027, 408310, 161408, 227187, 9951, 363428, 122210, 196270, 78031, 310016, 182181, 271699, 225261, 265165, 227187, 9951, 170285, 73314, 133848, 1703, 227089, 21559, 196270, 341104, 118502, 196270, 407944, 138705, 375123, 249499, 68468, 104532, 224651, 227187, 354132, 78031, 31355, 227089, 375925, 331324, 133848, 170285, 19233, 407944, 138705, 308713, 375123, 381079, 246135, 91549, 408310, 133848, 48121, 170285, 80798, 31892, 133848, 89854, 227187, 97229, 9951, 409543, 271164, 1703, 227089, 375123, 70317, 375123, 133848, 121195, 196270, 184484, 1703, 301456, 331324, 310456, 375123, 271771, 375123, 408310, 123705, 227187, 134843, 64529, 227089, 126663, 87063, 129906, 375123, 278046, 106006, 409543, 170285, 406438, 227187, 258170, 280512, 271164, 366787, 286956, 346101, 328907, 255273, 196270, 174985, 19580, 133848, 53740, 227187, 105032, 9951, 3492, 375123, 104532, 99754, 375123, 227089, 78031, 346739, 271164, 1703, 396578, 65100, 78031, 314374, 36913, 327499, 138705, 227187, 284730, 283592, 19233, 27770, 186371, 117302, 1703, 308713, 227089, 78031, 229296, 401322, 196270, 41159, 408310, 66292, 57661, 396578, 313035, 286956, 123705, 331324, 236570, 133848, 190154, 196270, 392407, 375123, 349735, 375123, 420210, 375123, 143817, 408310, 133848, 73061, 117302, 78031, 407183, 227187, 1703, 227089, 27770, 389351, 363255, 203862, 331324, 236570, 133848, 426063, 196270, 170452, 407092, 227187, 1703, 323783, 133848, 190154, 196270, 392407, 375123, 420210, 375123, 408310, 313035, 408310, 133848, 426063, 196270, 407092, 179355, 1703, 259633, 97229, 299495, 87063, 209465, 331324, 57661, 205336, 365557, 134843, 391441, 78031, 265165, 375123, 372121, 361133, 424298, 396578, 205668, 286956, 203682, 365557, 258170, 159433, 77027, 331324, 236252, 408310, 46156, 426517, 396578, 286956, 315144, 111980, 331324, 360530, 365557, 284730, 283592, 19233, 27770, 186371, 117302, 1703, 308713, 227089, 78031, 229296, 401322, 196270, 41159, 408310, 66292, 57661, 396578, 313035, 286956, 123705, 331324, 236570, 133848, 190154, 196270, 392407, 375123, 349735, 375123, 420210, 375123, 143817, 408310, 133848, 73061, 117302, 78031, 407183, 227187, 1703, 227089, 27770, 389351, 363255, 203862, 331324, 236570, 133848, 426063, 196270, 170452, 407092, 227187, 39475, 265506, 409225, 12457, 1703, 227187, 265506, 155309, 396170, 86747, 21967, 196270, 332026, 19580, 392407, 375123, 420210, 408310, 133848, 73061, 117302, 78031, 407183, 396578, 78031, 163158, 196270, 348461, 227089, 65232, 29469, 331324, 236570, 227187, 41116, 390367, 259633, 159421, 158981, 12457, 388371, 226436, 375123, 274337, 159421, 403242, 334130, 19580, 188472, 375123, 274337, 104532, 159421, 410316, 36483, 408310, 215220, 320300, 274337, 227187, 1703, 156681, 78031, 402840, 375123, 323055, 424298, 331324, 113891, 313035, 408310, 203862, 43863, 44816, 182635, 88638, 136511, 331324, 66292, 57661, 331324, 400075, 136511, 227187, 64529, 227089, 78031, 265165, 375123, 372121, 361133, 408169, 396578, 205668, 286956, 203682, 227187, 1703, 55946, 12457, 131966, 408310, 93812, 103743, 375123, 108209, 24767, 227187, 9951, 159421, 264718, 274337, 104532, 31060, 29078, 271699, 89946, 331324, 267639, 346173, 43635, 72689, 59027, 29078, 271699, 65232, 86747, 78031, 385346, 73061, 258614, 104532, 271699, 263212, 86747, 434413, 428604, 295440, 104532, 168040, 227187, 1703, 323783, 133848, 190154, 196270, 392407, 375123, 420210, 375123, 408310, 313035, 408310, 133848, 426063, 196270, 407092, 179355, 1703, 259633, 97229, 299495, 87063, 209465, 331324, 57661, 205336, 365557, 134843, 391441, 78031, 265165, 375123, 372121, 361133, 424298, 396578, 205668, 286956, 203682, 365557, 258170, 159433, 77027, 331324, 236252, 408310, 46156, 426517, 396578, 286956, 315144, 111980, 331324, 360530, 365557, 299821, 249071, 155417, 39416, 227187, 404999, 350798, 19233, 1703, 308713, 227089, 78031, 32924, 196270, 417212, 238069, 42421, 196270, 278983, 331324, 350798, 113443, 364615, 86747, 133848, 24882, 396578, 263353, 331324, 78031, 440668, 147936, 46078, 259341, 365238, 375349, 353638, 125300, 227187, 404999, 350798, 286956, 315144, 247685, 117302, 293230, 302980, 104532, 250143, 227187, 212312, 1703, 113443, 202500, 133848, 163600, 350798, 19580, 395968, 291, 296466, 363255, 78031, 440668, 419578, 365557, 133848, 217760, 286784, 396578, 133848, 347745, 179373, 352323, 86747, 183182, 133848, 419578, 227089, 227187, 410316, 310016, 182181, 19233, 1703, 308713, 375123, 396682, 66152, 331324, 117302, 73557, 78031, 159421, 310016, 375123, 274337, 227089, 16472, 78031, 346646, 118502, 196270, 182181, 389351, 365238, 133848, 103651, 388177, 408310, 51417, 303143, 396578, 30156, 331324, 78031, 236942, 272510, 227187, 97229, 9951, 409543, 271164, 1703, 227089, 375123, 70317, 375123, 133848, 121195, 196270, 184484, 1703, 301456, 331324, 310456, 375123, 271771, 375123, 408310, 123705, 227187, 134843, 64529, 227089, 126663, 87063, 129906, 375123, 278046, 106006, 409543, 170285, 406438, 227187, 258170, 280512, 271164, 366787, 286956, 346101, 328907, 255273, 196270, 174985, 19580, 133848, 53740, 227187, 105032, 9951, 3492, 375123, 104532, 99754, 375123, 227089, 78031, 346739, 271164, 1703, 396578, 65100, 78031, 314374, 36913, 327499, 138705, 227187, 126026, 89161, 181490, 16058, 227187, 1703, 227089, 78031, 155714, 408310, 260081, 273851, 12457, 200939, 365238, 296440, 375123, 329184, 408310, 260081, 227187, 302107, 103980, 196270, 263014, 113891, 14530, 310045, 154127, 135261, 155714, 157936, 149457, 363255, 54508, 260081, 352162, 227187]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Par2:\n"
      ],
      "metadata": {
        "id": "HPq9N6VisRMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def par_tf_idf_vectorizer(ind , npar):\n",
        "  w = par_tf_idf(ind , npar)\n",
        "  vec = np.zeros(len(V))\n",
        "  for weight , word in zip(w , preprocess(Splited_list[ind][npar])):\n",
        "    vec[VocabIndex[word]] = weight\n",
        "  return vec"
      ],
      "metadata": {
        "id": "mlRlgcZOj3kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc_tf_idf_vectorizer(ind):\n",
        "  w = tf_idf(ind)\n",
        "  vec = np.zeros(len(V))\n",
        "  for weight , word in zip(w , Tokenized_list[ind]):\n",
        "    vec[VocabIndex[word]] = weight\n",
        "  return vec"
      ],
      "metadata": {
        "id": "HQ1PmxEZ0BuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import get_close_matches\n",
        "def find_most_similar_word(query, stored_words):\n",
        "    cms = get_close_matches(query, stored_words)\n",
        "    if len(cms) > 0:\n",
        "        return cms[0]\n",
        "\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "V4EMDHmUTLoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Query_tfidf_vectorizer(query):\n",
        "  tokenized_query = preprocess(query)\n",
        "  query =[find_most_similar_word(word, V) for word in tokenized_query]\n",
        "\n",
        "  tf = Counter(query)\n",
        "  queryset = set(query)\n",
        "  vec = np.zeros(len(V))\n",
        "\n",
        "  w = []\n",
        "  for i in query:\n",
        "    if IDF[i] == 0:\n",
        "      print(i)\n",
        "      continue\n",
        "    w.append(np.log10(1 +tf[i]) * np.log10(float(len(Tokenized_list)) / float(IDF[i])))\n",
        "\n",
        "  for weight , word in zip(w , query):\n",
        "    vec[VocabIndex[word]] = weight\n",
        "\n",
        "  return vec"
      ],
      "metadata": {
        "id": "eVJ03f-sZzNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = par_tf_idf_vectorizer(3 , 2)\n",
        "x = np.argsort(v)[::-1][0:5]\n",
        "print(v[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAI-kS6cyKRd",
        "outputId": "05adfeea-7e9a-4a22-afe9-565b7020c5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.03161713 1.58375394 1.13106321 1.07034986 0.67463736]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = doc_tf_idf_vectorizer(3)\n",
        "x = np.argsort(v)[::-1][0:5]\n",
        "print(v[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyyAtYt90OdO",
        "outputId": "313906d5-c8e2-4df7-e7b7-240d632029cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.78155687 2.69108353 2.59986964 2.21398733 2.1140093 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = Query_tfidf_vectorizer(\"The most expendive patents are international patents\")\n",
        "x = np.argsort(v)[::-1][0:5]\n",
        "print(v[x])\n",
        "# expendive have been trasnformed to expensive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgG_YKXWcj_2",
        "outputId": "2655941c-a2eb-47f8-8365-7ae2f0ef4e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['expensive', 'patents', 'international', 'patents']\n",
            "[1.3558098  0.41829968 0.41525771 0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Part3:"
      ],
      "metadata": {
        "id": "UnlbtuLusXnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return dot_product / (norm_a * norm_b)"
      ],
      "metadata": {
        "id": "ghCwhKCBw5Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/data.json', 'r') as file:\n",
        "    Queries = json.load(file)\n",
        "query = Queries[0]['query']\n",
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4lSAIROf8B-",
        "outputId": "e6440e64-975f-4233-ac87-54fe687ccb5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(54000 , 54016):\n",
        "\n",
        "  print(f\"Query_id:{i}\")\n",
        "  query = Queries[i]['query']\n",
        "  candidates = Queries[i]['candidate_documents_id']\n",
        "  print(f\"Query:{query}\")\n",
        "  print(f\"Candidates:{candidates}\")\n",
        "  ans = Queries[i]['document_id']\n",
        "  print(f\"Query Answer: {ans}\")\n",
        "  parlist = Queries[i]['is_selected']\n",
        "  anspar = 0\n",
        "  for x , y in enumerate(parlist):\n",
        "    if y == 1:\n",
        "      anspar = x\n",
        "  print(f\"Ans is in Sentence:{anspar}\")\n",
        "\n",
        "\n",
        "  queryvec = Query_tfidf_vectorizer(query)\n",
        "\n",
        "  x = np.argsort(queryvec)[::-1][0:5]\n",
        "\n",
        "  sims = []\n",
        "  for j , can in enumerate(candidates):\n",
        "    docvec = doc_tf_idf_vectorizer(can)\n",
        "    x = np.argsort(docvec)[::-1][0:5]\n",
        "\n",
        "    sims.append( (cosine_similarity(queryvec , docvec) , can))\n",
        "  sims = sorted(sims , reverse=True)\n",
        "  print(sims[:5])\n",
        "\n",
        "  print(f\"Most similar:{sims[0][1]}\")\n",
        "\n",
        "\n",
        "  ind = sims[0][1]\n",
        "  parsims = []\n",
        "  for j in range(len(Splited_list[ind])):\n",
        "    parvec = par_tf_idf_vectorizer(ind , j)\n",
        "    parsims.append((cosine_similarity(queryvec , parvec) , j))\n",
        "  print(parsims)\n",
        "  parsims = sorted(parsims , reverse=True)\n",
        "  print(\"Part Of Context:\")\n",
        "  print(Splited_list[ind][parsims[0][1]])\n",
        "  print(f\"Num sent find in:{parsims[0][1]}\")\n",
        "\n",
        "\n",
        "  print(\"--------------------------------------------------------\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "pdxpiZhUqshQ",
        "outputId": "dd02a672-cf2e-4e29-f8c8-d7e98c384918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query_id:54000\n",
            "Query:Genetically modified (GM) foods are foods derived from organisms whose genetic material (DNA) has been modified in a way that does not occur naturally.\n",
            "Candidates:[5004, 36066, 9706, 8080, 34543, 45159, 49779, 11050, 41256, 10273, 30225, 19105, 1675, 40948, 46812, 12884, 1651, 41457, 1031, 49225]\n",
            "Query Answer: 49225\n",
            "Ans is in Sentence:5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-3234a4755202>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mqueryvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuery_tfidf_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-115-ebd2d790b772>\u001b[0m in \u001b[0;36mQuery_tfidf_vectorizer\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mQuery_tfidf_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtokenized_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfind_most_similar_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_query\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-115-ebd2d790b772>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mQuery_tfidf_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtokenized_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfind_most_similar_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_query\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-adf8b0e266f8>\u001b[0m in \u001b[0;36mfind_most_similar_word\u001b[0;34m(query, stored_words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdifflib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_close_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_most_similar_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstored_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_close_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstored_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/difflib.py\u001b[0m in \u001b[0;36mget_close_matches\u001b[0;34m(word, possibilities, n, cutoff)\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seq2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibilities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seq1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_quick_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m            \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquick_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/difflib.py\u001b[0m in \u001b[0;36mset_seq1\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seq2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mset_seq1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \"\"\"Set the first sequence to be compared.\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random_numbers = random.sample(range(1, 50001), 40)\n",
        "cnt = 0\n",
        "for i in random_numbers:\n",
        "\n",
        "  query = Queries[i]['query']\n",
        "  candidates = Queries[i]['candidate_documents_id']\n",
        "  ans = Queries[i]['document_id']\n",
        "  parlist = Queries[i]['is_selected']\n",
        "  anspar = 0\n",
        "  for x , y in enumerate(parlist):\n",
        "    if y == 1:\n",
        "      anspar = x\n",
        "\n",
        "\n",
        "  queryvec = Query_tfidf_vectorizer(query)\n",
        "\n",
        "  x = np.argsort(queryvec)[::-1][0:5]\n",
        "\n",
        "  sims = []\n",
        "  for j , can in enumerate(candidates):\n",
        "    docvec = doc_tf_idf_vectorizer(can)\n",
        "    x = np.argsort(docvec)[::-1][0:5]\n",
        "\n",
        "    sims.append( (cosine_similarity(queryvec , docvec) , can))\n",
        "  sims = sorted(sims , reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "  ind = sims[0][1]\n",
        "  if ind == ans :\n",
        "    cnt += 1\n",
        "\n",
        "\n",
        "print(cnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0JgQqCmzj0S",
        "outputId": "d56cc6f3-1038-4513-c0eb-da228c23bc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-116-815798a08122>:7: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return dot_product / (norm_a * norm_b)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "80% accuracy"
      ],
      "metadata": {
        "id": "U7Qmi4A-2N9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part4:"
      ],
      "metadata": {
        "id": "5OX3zsq9uYnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stl = []\n",
        "for tl in Tokenized_list:\n",
        "  tf = Counter(tl)\n",
        "  stl.append(sorted(tf.items(), key=lambda x:x[1] , reverse = True))"
      ],
      "metadata": {
        "id": "aFmFijVst6Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part5:"
      ],
      "metadata": {
        "id": "YDqebOPGue20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top5(ind):\n",
        "  w = tf_idf(ind)\n",
        "  top_indices = np.argsort(w)\n",
        "  res = []\n",
        "  res.append(top_indices[-1])\n",
        "  for i in reversed(list(top_indices)):\n",
        "    if Tokenized_list[ind][i] != Tokenized_list[ind][res[-1]]:\n",
        "      res.append(i)\n",
        "    if len(res) == 5:\n",
        "      return res\n",
        "  return res\n",
        "for i in top5(0):\n",
        "  print(Tokenized_list[0][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KgerDEfujPH",
        "outputId": "e9587673-6bc7-455c-fd16-998faef28b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rba\n",
            "rebuildable\n",
            "atomizer\n",
            "authentication\n",
            "genny\n"
          ]
        }
      ]
    }
  ]
}